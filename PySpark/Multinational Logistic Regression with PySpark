# Regular modules
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import pyspark
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import pandas as pd
import re 
import string
get_ipython().run_line_magic('matplotlib', 'inline')
import keras
import folium


#spark sql imports
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import Row
from pyspark.sql.functions import UserDefinedFunction
from pyspark.sql.types import *
from pyspark.sql.functions import unix_timestamp, from_unixtime, to_timestamp, col, round, month, year, udf, date_format, to_date, datediff, lower

#spark ML imports
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import HashingTF, Tokenizer, Word2Vec, OneHotEncoder, StringIndexer, OneHotEncoderEstimator, StopWordsRemover
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator,  MulticlassMetrics
import bigdl
from pyspark.mllib.classification import LogisticRegressionWithLBFGS,SVMWithSGD
from pyspark.mllib.regression import LabeledPoint


#change configuration settings on Spark 
spark = SparkSession.builder.master('yarn-client').appName("local[*]").getOrCreate()
conf = spark.sparkContext._conf.setAll([("spark.sql.crossJoin.enabled", "true"),('spark.executor.memory', '8g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '32'), ('spark.cores.max', '32'), ('spark.driver.memory','15g'),("spark.jars.packages", "JohnSnowLabs:spark-nlp:2.1.0")])
sqlContext = pyspark.SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)


df_income = spark.read.csv('data/project/kaggle_income.csv',header=True, inferSchema="true")
df_income.printSchema()


# Where are the least/most wealthiest places to live?
df_income2 = df_income.groupby(["State_Name","City"]).agg({"Median":"avg"})
df_income2 = df_income2.withColumn("avg(Median)", round(df_income2["avg(Median)"],2).alias("median"))
df_income2.sort(col("avg(Median)").desc()).show(5)
df_income2.sort(col("avg(Median)").asc()).show(5)

# Find Average income by state.
df_income3 = df_income.groupby(["City","Zip_Code"]).agg({"Median":"avg"}).alias("median1")
df_income3 = df_income2.select(col("City").alias("City_match"), col("avg(Median)").alias("Median_match"))
df_income4 = df_income2.join(df_income3, df_income2.City== df_income3.City_match, how="left")
df_income5 = df_income4.groupby(["State_Name","City"]).agg({"avg(Median)":"Mean"})
df_income5.show(20)


# Import all yelp data
# Find out which states are people yelping the most and make a heat map of the USA.
df_bus = sqlContext.read.json('user/adhamsuliman/data/business.json').dropna(thresh=1,subset=('state','city','business_id',"longitude","latitude"))
df_bus.printSchema()
df_bus.groupby("state").count().sort(col("count").desc()).show(50)
df_checkin = sqlContext.read.json('user/adhamsuliman/data/checkin.json').dropna(thresh=1, subset='business_id')
df_checkin.printSchema()
df_review = sqlContext.read.json('user/adhamsuliman/data/review.json').dropna(thresh=1, subset=('stars','business_id'))
df_review.printSchema()
df_tip = sqlContext.read.json('user/adhamsuliman/data/tip.json').dropna(thresh=1, subset=('business_id','user_id'))
df_tip.printSchema()
df_user = sqlContext.read.json('user/adhamsuliman/data/user.json')
df_user.printSchema()


# Filter tables for strictly restraurnats 
df_restaurants = df_bus.filter(df_bus.categories.like('%Restaurants%')|df_bus.categories.like('%Food%'))
df_restaurants_match = df_restaurants.groupby(["City"]).agg(F.count('address'))
df_income6 = df_income5.select(col("City").alias("City_inc"), col("avg(avg(Median))"))
df_res_inc = df_income6.join(df_restaurants_match, df_restaurants_match.City == df_income6.City_inc, how="inner" )
df_res_inc1 = df_res_inc.select(col("City"),col("avg(avg(Median))").alias("Median"),col("count(address)").alias("count")).sort(col("count").asc()) #.show(100)


# Create a visualization for restaurants displaying their review rating for the city of Las Vegas
import folium.plugins as plugins
rating_data = df_restaurants.toPandas()
lat = 36.207430
lon = -115.268460
lon_min, lon_max = lon-0.3,lon+0.5
lat_min, lat_max = lat-0.4,lat+0.5
#subset for vegas
ratings_data_vegas=rating_data[(rating_data["longitude"]>lon_min) &                    (rating_data["longitude"]<lon_max) &                    (rating_data["latitude"]>lat_min) &                    (rating_data["latitude"]<lat_max)]


data=[]
#rearranging data to suit the format needed for folium
stars_list=list(rating_data['stars'].unique())
for star in stars_list:
    subset=ratings_data_vegas[ratings_data_vegas['stars']==star]
    data.append(subset[['latitude','longitude']].values.tolist())
#initialize at vegas
lat = 36.127430
lon = -115.138460
zoom_start=11
print("                     Vegas Review heatmap Animation ")

# basic map
m = folium.Map(location=[lat, lon], tiles="OpenStreetMap", zoom_start=zoom_start)
#inprovising the Heatmapwith time plugin to show variations across star ratings 
hm = plugins.HeatMapWithTime(data,max_opacity=0.3,auto_play=True,display_index=True,radius=7)
hm.add_to(m)
m


# Create a spark Dataframe filtered on Las Vegas and Charlotsville, Vermont 
df_restaurants1 = df_restaurants.filter(df_restaurants.city.like('%Las Vegas%')|df_restaurants.city.like('%Charlottesville%'))
df_review2 = df_review.select(col("business_id"), col("date").alias("review_date"),col("stars").alias("review_stars"), col("text").alias("review_text"))
df_res_review = df_restaurants1 .join(df_review2, df_restaurants.business_id == df_review2.business_id, how="inner"  )
df_res_review.show(5)

# NLP
# Apply lower case to text
df_res_review1 = df_res_review.select("review_text","review_stars", "city") 
df_res_review1 = df_res_review1.withColumn("review_text",lower(col('review_text')))


# Tokenize text
tokenizer = Tokenizer(inputCol="review_text", outputCol="words")
df_res_review1 = tokenizer.transform(df_res_review1)

# Remove stop words from text
remover = StopWordsRemover(inputCol="words", outputCol="filtered")
df_res_review2 = remover.transform(df_res_review1)
df_res_review2.collect()

# Create Word2Vec model
word2Vec = Word2Vec(vectorSize=20, minCount=10, inputCol="filtered", outputCol="wordVectors")
w2VM = word2Vec.fit(df_res_review2)
nlpdf = w2VM.transform(df_res_review2)
#nlpdf = nlpdf.filter(nlpdf.stars.isNotNull())
nlpdf = nlpdf.filter(nlpdf.wordVectors.isNotNull())



nlpdf = nlpdf.withColumn('review_stars',nlpdf.review_stars-1)
nlpdf = nlpdf.select("review_stars","wordVectors","city")

# Create a feature column which combines wordVectors and City
from pyspark.ml.feature import RFormula
rf = RFormula(formula="review_stars  ~ wordVectors ") #+ city
final_df_rf = rf.fit(nlpdf).transform(nlpdf)
final_df_rf1 = final_df_rf.select("features","label")


# Create an RDD with LabeledPoint to prepare for multinomial logistic Regression
nlpdf1 = final_df_rf1.rdd
nlpdf2 = nlpdf1.map(lambda line: LabeledPoint(line[1],[line[0]]))


# Split into train and test
splits = nlpdf2.randomSplit([0.8, 0.2])
train_df = splits[0]
test_df = splits[1]

# Train the model and comput classification score on test set
model = LogisticRegressionWithLBFGS.train(train_df, numClasses=9)
predictionAndLabels = test_df.map(lambda lp: (float(model.predict(lp.features)), lp.label))
metrics = MulticlassMetrics(predictionAndLabels)
accuracy = metrics.accuracy
print("Summary Stats")
print("Accuracy = %s" % accuracy)

