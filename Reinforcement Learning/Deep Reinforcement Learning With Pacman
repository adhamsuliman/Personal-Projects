import numpy as np
import random
import math
import json


import matplotlib.pyplot as plt
from mini_pacman_original import PacmanGame
import os
import pyglet 

import random
import gc
import time
import numpy as np

from keras.models import Sequential, clone_model
from keras.layers import Dense, InputLayer
from keras.optimizers import Adam
from keras.callbacks import CSVLogger, TensorBoard
import keras.backend as K

import json

with open('test_params.json', 'r') as file:
    read_params = json.load(file)
game_params = read_params['params']

from collections import deque

env = PacmanGame(**game_params)

# Create the neural network for both our online and target network.
def create_dqn_model(input_shape, nb_actions, dense_layers, dense_units):
    model = Sequential()
    model.add(InputLayer(input_shape=input_shape))
    for i in range(dense_layers):
        model.add(Dense(units=dense_units, activation='relu'))
    model.add(Dense(nb_actions, activation='linear'))
    return model
    
obs = env.reset()

# Create the hyper-parameteres for the neural network
input_shape = (34,)# obs.shape
nb_actions = 9  # 9
dense_layers = 5
dense_units = 256

# Create the online network
online_network = create_dqn_model(input_shape, nb_actions, dense_layers, dense_units)
online_network.summary()


# Create teh strategy teh onlien network will use to determine it's next decision. 
def strategy(q_values, epsilon, n_outputs, possible_actions=env.player_possible_actions()):
    if random.random() < epsilon:
        return random.choice(possible_actions)  # random action
    else:
        #print(possible_actions)
        #possible_actions = [i-1 for i in possible_actions]
        return  np.argmax([q_values[i] for i in possible_actions])          # q-optimal action

# Create a list which will be used as the online netowrk's "memory"
replay_memory_maxlen = 1000000
replay_memory = deque([], maxlen=replay_memory_maxlen)

# Create the target network
target_network = clone_model(online_network)
target_network.set_weights(online_network.get_weights())



# Create the hyperparameters which will be used in training. 
name = 'MsPacman_DQN'  # used in naming files (weights, logs, etc)
n_steps = 7000000        # total number of training steps (= n_epochs)
warmup = 5000       # start training after warmup iterations
training_interval = 10  # period (in actions) between training steps
save_steps = int(n_steps/100)  # period (in training steps) between storing weights to file
copy_steps = 5000    # period (in training steps) between updating target_network weights
gamma = 0.95           # discount rate
skip_start = 90 # skip the start of every game (it's just freezing time before game starts)
batch_size = 32        # size of minibaatch athat is taken randomly from replay memory every training step
double_dqn = False     # whether to use Double-DQN approach or simple DQN (see above)
# eps-greedy parameters: we slowly decrease epsilon from eps_max to eps_min in eps_decay_steps
eps_max = 1.0
eps_min = 0.05
eps_decay_steps = int(n_steps*.8)

learning_rate = 0.0005

def mean_q(y_true, y_pred):
    return K.mean(K.max(y_pred, axis=-1))


online_network.compile(optimizer=Adam(learning_rate), loss='mse', metrics=[mean_q])


if not os.path.exists(name):
    os.makedirs(name)
    
weights_folder = os.path.join(name, 'weights')
if not os.path.exists(weights_folder):
    os.makedirs(weights_folder)
    
csv_logger = CSVLogger(os.path.join(name, 'log.csv'), append=True, separator=';')
tensorboard = TensorBoard(log_dir=os.path.join(name, 'tensorboard'), write_graph=False, write_images=False)


# Create a function which will return the state of the environment after the online network has decided upon an action.
def get_state(obs):
    v = []
    x,y = obs['player']
    v.append(x)
    v.append(y)
    for x, y in obs['monsters']:
        v.append(x)
        v.append(y)
    for x, y in obs['diamonds']:
        v.append(x)
        v.append(y)
    for x, y in obs['walls']:
        v.append(x)
        v.append(y)
    for m in obs['monsters']: # added
        v.append(np.linalg.norm(np.array(obs['player'])-np.array(m))) # added    
    return v



# counters:
step = 1000          # training step counter (= epoch counter)
iteration = 500     # frames counter
episodes = 500     # game episodes counter
done = True       # indicator that env needs to be reset

episode_scores = []  # collect total scores in this list and log it later

while step < n_steps:
    if done:  # game over, restart it
        obs = env.reset()
        score = 0  # reset score for current episode
        for skip in range(skip_start):  # skip the start of each game (it's just freezing time before game starts)
            obs = env.get_obs() #info
            done = obs.get('end_game')
            score =+ obs.get('reward')
            
        state = get_state(env.get_obs())
        episodes += 1
    observation = env.get_obs()
    state = get_state(observation)
    #Online network evaluates what to do
    iteration += 1
    q_values = online_network.predict(np.array([state]))[0]  # calculate q-values using online network
    # select epsilon (which linearly decreases over training steps):
    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)
    action = strategy(q_values, epsilon, nb_actions) #obs = observation
    # Play:
    decision = env.make_action(action) #info
    obs = env.get_obs()
    done = obs.get('end_game')
    score =+ obs.get('reward')
    reward = obs.get('reward')
    score += reward
    if done:
        episode_scores.append(score)
    next_state = obs
    # Let's memorize what just happened
    replay_memory.append((state, action, reward, next_state, done))
    
    if iteration >= warmup and iteration % training_interval == 0:
        # learning branch
        step += 1
        minibatch = random.sample(replay_memory, batch_size)
        replay_state = np.array([x[0] for x in minibatch])
        replay_action = np.array([x[1] for x in minibatch])
        replay_rewards = np.array([x[2] for x in minibatch])
        replay_next_state = np.array([get_state(x[3]) for x in minibatch])
        replay_done = np.array([x[4] for x in minibatch], dtype=int)
        
        
        if double_dqn == False:
            # DQN
            target_for_action = replay_rewards + (1-replay_done) * gamma * \
                                    np.amax(target_network.predict(replay_next_state), axis=1)
        else:
            # Double DQN
            best_actions = np.argmax(online_network.predict(replay_next_state), axis=1)
            target_for_action = replay_rewards + (1-replay_done) * gamma * \
                                    target_network.predict(replay_next_state)[np.arange(batch_size), best_actions]
       
        target = online_network.predict(replay_state)# targets coincide with predictions ...
        target[np.arange(batch_size), replay_action] = target_for_action  #...except for targets with actions from replay
        
        # Train online network
        online_network.fit(replay_state, target, epochs=step, verbose=2, initial_epoch=step-1,
                           callbacks=[csv_logger])

        # Periodically copy online network weights to target network
        if step % copy_steps == 0:
            target_network.set_weights(online_network.get_weights())
        # And save weights
        if step % save_steps == 0:
            online_network.save_weights(os.path.join(weights_folder, 'weights_{}.h5f'.format(step)))
            gc.collect()  # also clean the garbage

    
online_network.save_weights(os.path.join(weights_folder, 'weights_last.h5f'))    
