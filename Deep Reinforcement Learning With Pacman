import numpy as np
import random
import math
import json


import matplotlib.pyplot as plt
from mini_pacman_original import PacmanGame
import os
import pyglet 

import random
import gc
import time
import numpy as np

from keras.models import Sequential, clone_model
from keras.layers import Dense, InputLayer
from keras.optimizers import Adam
from keras.callbacks import CSVLogger, TensorBoard
import keras.backend as K

import json

with open('test_params.json', 'r') as file:
    read_params = json.load(file)
game_params = read_params['params']

from collections import deque

env = PacmanGame(**game_params)


def create_dqn_model(input_shape, nb_actions, dense_layers, dense_units):
    model = Sequential()
    model.add(InputLayer(input_shape=input_shape))
    for i in range(dense_layers):
        model.add(Dense(units=dense_units, activation='relu'))
    model.add(Dense(nb_actions, activation='linear'))
    return model
    
obs = env.reset()
input_shape = (34,)# obs.shape
nb_actions = 9  # 9
dense_layers = 5
dense_units = 256

online_network = create_dqn_model(input_shape, nb_actions, dense_layers, dense_units)
online_network.summary()



def strategy(q_values, epsilon, n_outputs, possible_actions=env.player_possible_actions()):
    if random.random() < epsilon:
        return random.choice(possible_actions)  # random action
    else:
        #print(possible_actions)
        #possible_actions = [i-1 for i in possible_actions]
        return  np.argmax([q_values[i] for i in possible_actions])          # q-optimal action
    
replay_memory_maxlen = 1000000
replay_memory = deque([], maxlen=replay_memory_maxlen)

target_network = clone_model(online_network)
target_network.set_weights(online_network.get_weights())




name = 'MsPacman_DQN'  # used in naming files (weights, logs, etc)
n_steps = 7000000        # total number of training steps (= n_epochs)
warmup = 5000       # start training after warmup iterations
training_interval = 10  # period (in actions) between training steps
save_steps = int(n_steps/100)  # period (in training steps) between storing weights to file
copy_steps = 5000    # period (in training steps) between updating target_network weights
gamma = 0.95           # discount rate
skip_start = 90 # skip the start of every game (it's just freezing time before game starts)
batch_size = 32        # size of minibaatch athat is taken randomly from replay memory every training step
double_dqn = False     # whether to use Double-DQN approach or simple DQN (see above)
# eps-greedy parameters: we slowly decrease epsilon from eps_max to eps_min in eps_decay_steps
eps_max = 1.0
eps_min = 0.05
eps_decay_steps = int(n_steps*.8)

learning_rate = 0.0005

def mean_q(y_true, y_pred):
    return K.mean(K.max(y_pred, axis=-1))


online_network.compile(optimizer=Adam(learning_rate), loss='mse', metrics=[mean_q])


if not os.path.exists(name):
    os.makedirs(name)
    
weights_folder = os.path.join(name, 'weights')
if not os.path.exists(weights_folder):
    os.makedirs(weights_folder)
    
csv_logger = CSVLogger(os.path.join(name, 'log.csv'), append=True, separator=';')
tensorboard = TensorBoard(log_dir=os.path.join(name, 'tensorboard'), write_graph=False, write_images=False)



def get_state(obs):
    v = []
    x,y = obs['player']
    v.append(x)
    v.append(y)
    for x, y in obs['monsters']:
        v.append(x)
        v.append(y)
    for x, y in obs['diamonds']:
        v.append(x)
        v.append(y)
    for x, y in obs['walls']:
        v.append(x)
        v.append(y)
    for m in obs['monsters']: # added
        v.append(np.linalg.norm(np.array(obs['player'])-np.array(m))) # added    
    return v



# counters:
step = 1000          # training step counter (= epoch counter)
iteration = 500     # frames counter
episodes = 500     # game episodes counter
done = True       # indicator that env needs to be reset

episode_scores = []  # collect total scores in this list and log it later

while step < n_steps:
    if done:  # game over, restart it
        obs = env.reset()
        score = 0  # reset score for current episode
        for skip in range(skip_start):  # skip the start of each game (it's just freezing time before game starts)
            obs = env.get_obs() #info
            done = obs.get('end_game')
            score =+ obs.get('reward')
            
        state = get_state(env.get_obs())
        episodes += 1
    observation = env.get_obs()
    state = get_state(observation)
    #Online network evaluates what to do
    iteration += 1
    q_values = online_network.predict(np.array([state]))[0]  # calculate q-values using online network
    # select epsilon (which linearly decreases over training steps):
    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)
    action = strategy(q_values, epsilon, nb_actions) #obs = observation
    # Play:
    decision = env.make_action(action) #info
    obs = env.get_obs()
    done = obs.get('end_game')
    score =+ obs.get('reward')
    reward = obs.get('reward')
    score += reward
    if done:
        episode_scores.append(score)
    next_state = obs
    # Let's memorize what just happened
    replay_memory.append((state, action, reward, next_state, done))
    
    if iteration >= warmup and iteration % training_interval == 0:
        # learning branch
        step += 1
        minibatch = random.sample(replay_memory, batch_size)
        replay_state = np.array([x[0] for x in minibatch])
        replay_action = np.array([x[1] for x in minibatch])
        replay_rewards = np.array([x[2] for x in minibatch])
        replay_next_state = np.array([get_state(x[3]) for x in minibatch])
        replay_done = np.array([x[4] for x in minibatch], dtype=int)
        
        
        if double_dqn == False:
            # DQN
            target_for_action = replay_rewards + (1-replay_done) * gamma * \
                                    np.amax(target_network.predict(replay_next_state), axis=1)
        else:
            # Double DQN
            best_actions = np.argmax(online_network.predict(replay_next_state), axis=1)
            target_for_action = replay_rewards + (1-replay_done) * gamma * \
                                    target_network.predict(replay_next_state)[np.arange(batch_size), best_actions]
       
        target = online_network.predict(replay_state)# targets coincide with predictions ...
        target[np.arange(batch_size), replay_action] = target_for_action  #...except for targets with actions from replay
        
        # Train online network
        online_network.fit(replay_state, target, epochs=step, verbose=2, initial_epoch=step-1,
                           callbacks=[csv_logger])

        # Periodically copy online network weights to target network
        if step % copy_steps == 0:
            target_network.set_weights(online_network.get_weights())
        # And save weights
        if step % save_steps == 0:
            online_network.save_weights(os.path.join(weights_folder, 'weights_{}.h5f'.format(step)))
            gc.collect()  # also clean the garbage

#Strategy1
def test_dqn(obs=obs, eps=.05, nb_actions = env.player_possible_actions()):
    q_values = online_network.predict(np.array([get_state(obs)]))[0]
    action = strategy(q_values, eps, nb_actions)
    return action 
    
online_network.save_weights(os.path.join(weights_folder, 'weights_last.h5f'))    

def preprocess(start_state):
    # make tuples from lists
    start_state['player'] = tuple(start_state['player'])
    start_state['monsters'] = [tuple(m) for m in start_state['monsters']]
    start_state['diamonds'] = [tuple(d) for d in start_state['diamonds']]
    start_state['walls'] = [tuple(w) for w in start_state['walls']]

#def test(strategy, log_file='test_pacman_log.json'):
def test(strategy, log_file='test_pacman_log.json'):
    with open('test_params.json', 'r') as file:
        read_params = json.load(file)

    game_params = read_params['params']
    test_start_states = read_params['states']
    total_history = []
    total_scores = []

    env = PacmanGame(**game_params)
    for start_state in test_start_states:
        preprocess(start_state)
        episode_history = []
        env.reset()
        env.player = start_state['player']
        env.monsters = start_state['monsters']
        env.diamonds = start_state['diamonds']
        env.walls = start_state['walls']
        assert len(env.monsters) == env.nmonsters and len(env.diamonds) == env.ndiamonds and len(env.walls) == env.nwalls

        obs = env.get_obs()
        episode_history.append(obs)
        while not obs['end_game']:
            action = strategy(obs)  
            obs = env.make_action(action)
            episode_history.append(obs)
        total_history.append(episode_history)
        total_scores.append(obs['total_score'])
    mean_score = np.mean(total_scores)
    median_score = np.median(total_scores)
    with open(log_file, 'w') as file:
        json.dump(total_history, file)
    print("Your average score is {}, median is {}, saved log to '{}'. "
          "Do not forget to upload it for submission!".format(mean_score, median_score, log_file))
    return median_score
test(test_dqn)

# Dump all scores to txt-file
with open(os.path.join(name, 'episode_scores.txt'), 'w') as file:
    for item in episode_scores:
        file.write("{}\n".format(item))

print(episode_scores)